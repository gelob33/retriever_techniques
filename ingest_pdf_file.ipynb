{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "11509b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_pymupdf4llm import PyMuPDF4LLMLoader\n",
    "from langchain.text_splitter import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from ollama import AsyncClient\n",
    "from typing import List, Sequence, Any\n",
    "from db.db_connection_pool_using_pycopg2 import get_connection, release_connection, close_pool\n",
    "from db.db_connection_pool import get_engine, get_conn\n",
    "from pydantic import BaseModel, Field\n",
    "from typing import List, Optional, Dict\n",
    "from db.schema import Document_Chunk\n",
    "from openai import OpenAI\n",
    "from datetime import datetime\n",
    "from zoneinfo import ZoneInfo\n",
    "import requests\n",
    "import logging\n",
    "import asyncio\n",
    "import aiohttp\n",
    "from sqlalchemy import text\n",
    "from fastembed import TextEmbedding\n",
    "from dotenv import load_dotenv\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7bf06d4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv(override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c9aa979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert pdf to markdown\n",
    "# https://pymupdf.readthedocs.io/en/latest/pymupdf4llm/api.html#pymupdf4llm-api\n",
    "FOLDER_PATH = r\"C:\\Users\\aibag\\git_repo\\policy_wording\"\n",
    "\n",
    "FILE_NAME = \"state-home-comprehensive-contents-comprehensive-insurance-policy-wording-si6995-2-1224.PDF\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "176b1226",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(name)s: %(message)s\"\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "current_nz_datetime = datetime.now(tz=ZoneInfo(\"Pacific/Auckland\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "538c17a6",
   "metadata": {},
   "source": [
    "#### Ingest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28c1480a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-11 22:03:11,707 [INFO] __main__: Processing file: C:\\Users\\aibag\\git_repo\\policy_wording\\state-home-comprehensive-contents-comprehensive-insurance-policy-wording-si6995-2-1224.PDF\n",
      "2025-10-11 22:03:20,855 [INFO] __main__: Successfully processed file: state-home-comprehensive-contents-comprehensive-insurance-policy-wording-si6995-2-1224.PDF; Total Pages: 61\n"
     ]
    }
   ],
   "source": [
    "# function to load pdf file and convert a pdf file to a markdown file\n",
    "async def load_pdf_file(filepath:str, filename:str, mode:str =\"single\") -> List[Document]:\n",
    "\n",
    "    full_path = os.path.join(filepath, filename)\n",
    "    logger.info(f\"Processing file: {full_path}\")\n",
    "\n",
    "    if not os.path.exists(full_path):\n",
    "        raise FileNotFoundError(f\"File not found: {full_path}\")\n",
    "\n",
    "    if not filename.lower().strip().endswith(\".pdf\"):\n",
    "        raise TypeError (\"Invalid File Type; only PDFs are allowed.\")\n",
    "\n",
    "    # custom pages_delimiter to identify where are ends of pages in single mode \n",
    "    # page = load each page as a Document object; single = load entire PDF as a single Document object\n",
    "    doc_loader = PyMuPDF4LLMLoader(full_path \n",
    "                                   ,mode=mode\n",
    "                                   ,pages_delimiter=\"<<-- PAGE BREAK -->>\\n\\n\"\n",
    "                                   ,table_strategy=\"lines_strict\" # lines, text, lines_strict, lines_strict is default\n",
    "                                   #,page_separators=True\n",
    "                                  )\n",
    "                            \n",
    "    # lazy loading\n",
    "    docs = []\n",
    "    async for doc in doc_loader.alazy_load():\n",
    "        docs.append(doc)\n",
    "\n",
    "    logger.info(f\"Successfully processed file: {filename}; Total Pages: {docs[0].metadata[\"total_pages\"]}\")\n",
    "\n",
    "    return docs\n",
    "\n",
    "doc_obj =  await load_pdf_file(FOLDER_PATH, FILE_NAME, mode=\"single\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9241ae50",
   "metadata": {},
   "source": [
    "#### Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "855a013d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# functions to chunk/split a markdown file into chunks\n",
    "def chunk_header_splitter(doc_contents)->list[Document]:\n",
    "\n",
    "    headers_to_split_on = [\n",
    "        (\"#\", \"Header 1\"),\n",
    "        (\"##\", \"Header 2\"),\n",
    "        (\"###\", \"Header 3\"),\n",
    "        ('####', \"Header 4\"),\n",
    "        ('#####', \"Header 5\"),\n",
    "        ('######', \"Header 6\"),\n",
    "        ('#######', \"Header 7\")\n",
    "    ]\n",
    "\n",
    "    markdown_splitter = MarkdownHeaderTextSplitter(headers_to_split_on, strip_headers=False)\n",
    "\n",
    "    return markdown_splitter.split_text(doc_contents)\n",
    "\n",
    "def chunk_header_recursivesplitter(doc_contents)->list:\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        separators=[\". \", \"! \", \"? \", \"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    "        chunk_size=2000,\n",
    "        chunk_overlap=200,\n",
    "    )\n",
    "    chunks = text_splitter.split_text(doc_contents)\n",
    "    return chunks\n",
    "\n",
    "#print(chunk_header_splitter(doc_obj[0].page_content))\n",
    "#print(chunk_header_recursivesplitter(doc[0].page_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a90ed7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model of the vector db\n",
    "class DocumentChunk(BaseModel):\n",
    "    embedding: List[float]\n",
    "    chunk_text: str\n",
    "    metadata: Optional[Dict[str, str]] = Field(default_factory=dict)\n",
    "    file_name: str\n",
    "    tags: Optional[List[str]] = Field(default_factory=list)\n",
    "    isActive: bool = Field(default=False)\n",
    "    chunk_enrichment: str = None\n",
    "    version: Optional[str] = None\n",
    "    created_at: Optional[datetime] = None\n",
    "    updated_at: Optional[datetime] = None\n",
    "\n",
    "# model for the llm enrichment output\n",
    "class ChunkEnrichment(BaseModel):\n",
    "    chunk_summary: str\n",
    "    hypotetical_questions: list[str]\n",
    "    has_table: bool = Field(default=False)\n",
    "    table_summary: Optional[str] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5121ba2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-11 22:03:20,886 [INFO] __main__: Chunk Size for document 'state-home-comprehensive-contents-comprehensive-insurance-policy-wording-si6995-2-1224.PDF': 135\n"
     ]
    }
   ],
   "source": [
    "def chunk_document(doc_obj)->list[str]:\n",
    "\n",
    "    # get metadata from the doc object \n",
    "    source_path = doc_obj[0].metadata.get(\"source\", \"\")\n",
    "    file_name = source_path.split(\"\\\\\")[-1] if source_path else \"unknown\"\n",
    "\n",
    "    doc_metadata = {\n",
    "        \"source\"        : doc_obj[0].metadata.get(\"source\"),\n",
    "        \"file_name\"     : file_name,\n",
    "        \"total_pages\"   : str(doc_obj[0].metadata.get(\"total_pages\")),\n",
    "        \"creation_date\" : doc_obj[0].metadata.get(\"creationdate\"),\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        chunks = chunk_header_splitter(doc_obj[0].page_content)\n",
    "        logger.info(f\"Chunk Size for document '{file_name}': {len(chunks)}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Call to chunk_header_splitter failed: {e}\")    \n",
    "\n",
    "    doc_chunks_list = []\n",
    "\n",
    "    for chunk in chunks:\n",
    "\n",
    "        # skip pages that only have 30 char or less\n",
    "        if len(chunk.page_content) < 30: \n",
    "            continue \n",
    "\n",
    "        # additional metadata \n",
    "        chunk_metadata = doc_metadata.copy()\n",
    "\n",
    "        header_key, header_value = next(iter(chunk.metadata.items()), (None, None))\n",
    "\n",
    "        chunk_metadata[\"header_key\"] = header_key \n",
    "        chunk_metadata[\"header_value\"] = header_value.replace(\"*\",\"\").replace(\"#\",\"\")\n",
    "\n",
    "        # create an instance of DocumentChunk\n",
    "        doc_chunk = DocumentChunk(\n",
    "            embedding = [],\n",
    "            chunk_text = chunk.page_content,\n",
    "            metadata = chunk_metadata,\n",
    "            file_name = file_name,\n",
    "            isActive = True,\n",
    "            chunk_enrichment = \"\",\n",
    "            version = \"1\",\n",
    "            tags = [\"Home\", \"State\"],\n",
    "            created_at = current_nz_datetime,\n",
    "            updated_at= current_nz_datetime,\n",
    "        )\n",
    "\n",
    "        doc_chunks_list.append(doc_chunk)\n",
    "\n",
    "    return doc_chunks_list\n",
    "\n",
    "doc_chunks = chunk_document(doc_obj)\n",
    "#print(chunk_document(doc_obj))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd08809",
   "metadata": {},
   "source": [
    "#### Enrich Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156db80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# enrich each chunks \n",
    "# https://www.anthropic.com/engineering/contextual-retrieval\n",
    "enrichment_system_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "You are an expert general insurance underwriting product manager.  \n",
    "You will be given a document and a chunk of text from the document:\n",
    "\n",
    "Please give a short context to situate this chunk within the overall document for\n",
    "the purposes of improving search retrieval of the chunk.\n",
    "\n",
    "ONLY IF the chunk includes a table where you will provide a summary of the table.\n",
    "\n",
    "Also provide 3 to 5 hypotetical questions that the chunk will able to answer.\n",
    "\n",
    "Answer only with the succinct context and a list of hypotetical questions, nothing else. \n",
    "\n",
    "Here's the document:\n",
    "{document_text}\n",
    "\"\"\")\n",
    "\n",
    "enrichment_user_prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Here's the chunk:\n",
    "{chunk_text}\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a282cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# invoke - local lm studio llm model\n",
    "def invoke_llm_using_openai(system_prompt, query_prompt):\n",
    "    client = OpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"lmstudio\")\n",
    "\n",
    "    response = client.chat.completions.parse(\n",
    "        model= \"google/gemma-3-4b\",  \n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": query_prompt}\n",
    "        ],\n",
    "        temperature=0.0,\n",
    "        max_tokens=500,\n",
    "        top_p=0.9,\n",
    "        response_format = ChunkEnrichment,\n",
    "    )\n",
    "\n",
    "\n",
    "    #return response.choices[0].message.content\n",
    "    return response.choices[0].message.parsed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9bd1e939",
   "metadata": {},
   "outputs": [],
   "source": [
    "# async invoke - local lm studio llm model\n",
    "async def ainvoke_llm_api_generate(prompt):\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        async with session.post(\n",
    "            \"http://localhost:11434/api/generate\",\n",
    "\n",
    "            json={\n",
    "                \"model\": \"gemma3:4b\",\n",
    "                \"prompt\": prompt,\n",
    "                \"stream\": False,\n",
    "                \"temperature\": 0,\n",
    "                \"top_p\": 0.90,\n",
    "            }\n",
    "        ) as response:\n",
    "            resp_json = await response.json()\n",
    "            return resp_json[\"response\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "01ab7a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Ollama Model\n",
    "# https://github.com/ollama/ollama-python\n",
    "async def invokeOllamaModel(system_prompt, query_prompt, formatModel):\n",
    "    model='gemma3:4b'\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": query_prompt}\n",
    "    ]\n",
    "    response = await AsyncClient().chat(model=model, messages=messages, format=formatModel.model_json_schema())\n",
    "    response_text = formatModel.model_validate_json(response.message.content)\n",
    "    return response_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8771349e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-11 22:31:11,302 [INFO] httpx: HTTP Request: POST http://127.0.0.1:11434/api/chat \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk_summary='This is an introductory paragraph from State Insurance, thanking the customer and explaining the importance of reading the policy wording and schedule for claim information.' hypotetical_questions=[\"What does 'policy wording' refer to?\", 'Why is it important to keep the policy schedule and wording handy?', 'What happens if I need to make a claim?'] has_table=False table_summary=None\n"
     ]
    }
   ],
   "source": [
    "system_prompt = enrichment_system_prompt.format(document_text=doc_obj[0].page_content)\n",
    "query_prompt = enrichment_user_prompt.format(chunk_text=doc_chunks[0].chunk_text)\n",
    "\n",
    "response = await invokeOllamaModel(system_prompt, query_prompt, ChunkEnrichment)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67dcabd2",
   "metadata": {},
   "source": [
    "#### Chunk Enrichment and Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4732da",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = TextEmbedding(model_name=\"BAAI/bge-base-en-v1.5\")  \n",
    "\n",
    "def invoke_embedding(chunk_text):\n",
    "    embeddings = embedding_model.embed(chunk_text)\n",
    "    # TextEmbedding.embed returns a generator so I used next() \n",
    "    return next(embeddings) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a1a827",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Enriching Chunks:   0%|          | 0/134 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# Progress bar for chunk enrichment within the current file\n",
    "system_prompt = enrichment_system_prompt.format(document_text = doc_obj[0].page_content)\n",
    "\n",
    "with tqdm(total=len(doc_chunks), desc=f\"Enriching Chunks\", leave=False) as pbar_chunks:\n",
    "    for idx, chunk in enumerate(doc_chunks):\n",
    "        \n",
    "        print(\"-\"*50)\n",
    "        print(chunk)\n",
    "\n",
    "        # enrich chunk using LLM\n",
    "        query_prompt =  enrichment_user_prompt.format(chunk_text = chunk.chunk_text)\n",
    "        enriched_text = invokeOllamaModel(system_prompt, query_prompt)\n",
    "\n",
    "        enriched_parts = []\n",
    "        if enriched_text:\n",
    "            # pre-pend to the chunk:\n",
    "            enriched_parts = [\n",
    "            f\"Chunk Summary: {enriched_text.chunk_summary}\\n\",\n",
    "            f\"Questions this chunk may answer: {\"\\n\".join(f\"- {q}\" for q in enriched_text.hypotetical_questions)}\\n\",\n",
    "            ]\n",
    "             \n",
    "            if enriched_text.has_table and enriched_text.table_summary:\n",
    "                enriched_parts.append(f\"Table Summary: {enriched_text.table_summary}\\n\")\n",
    "        else:\n",
    "            logger.warning(f\"Doc: {chunk.file_name} Chunk: {idx} failed enrichment.\")\n",
    "\n",
    "        enriched_parts.append(f\"Chunk Text:\\n{chunk.chunk_text}\")\n",
    "\n",
    "        enriched_chunk = \"\\n\".join(enriched_parts)\n",
    "\n",
    "        # embed enriched_chunk\n",
    "        embedded_chunk = invoke_embedding(enriched_chunk)\n",
    "        \n",
    "        # write to the DocumentChunk object\n",
    "        if embedded_chunk is not None and len(embedded_chunk) > 0:\n",
    "            chunk.embedding = embedded_chunk\n",
    "\n",
    "        if enriched_chunk:\n",
    "            chunk.chunk_enrichment = enriched_chunk            \n",
    "\n",
    "        # Update chunk-level progress bar\n",
    "        pbar_chunks.update(1)\n",
    "\n",
    "        if idx > 0: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6465c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Chunk Summary: This chunk discusses the concept of 'flow' or 'being in the zone,' a psychological state where individuals are fully immersed and engaged in an activity, experiencing heightened focus, enjoyment, and performance. It explains that flow is characterized by a balance between challenge and skill level; when challenges exceed skills, anxiety arises, while when skills exceed challenges, boredom occurs. The ideal scenario for flow is when the challenge slightly exceeds the individual's current skill level, pushing them to grow and improve.  The chunk also mentions several key elements of flow, including clear goals, immediate feedback, a sense of control, loss of self-consciousness, transformation of time perception (time seems to fly by), and intrinsic motivation.\\n\\nQuestions this chunk may answer: - Can you describe a personal experience where you felt 'in the zone' or experienced flow?\\n- How does the balance between challenge and skill contribute to achieving a state of flow?\\n- What are some strategies someone could use to create conditions that promote flow in their work or hobbies?\\n- Why is intrinsic motivation considered an important element of flow?\\n- Imagine a scenario where someone consistently feels anxious while performing a task. How might they adjust the challenge level to potentially enter a state of flow?\\n\\nChunk Text:\\n#### **Important information about your policy**  \\n‘You’ and ‘your’ mean any person or entity shown as the Insured in your policy schedule. If you have\\nContents Insurance, ‘you’ and ‘your’ include any partner of the Insured. ‘We’, ‘us’ and ‘our’ mean IAG  \\nNew Zealand Limited.\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_chunks[0].chunk_enrichment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ec190984",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using SQLALCHEMY\n",
    "# create the engine\n",
    "db_engine = get_engine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aba46aef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-25 20:57:32,639 INFO sqlalchemy.engine.Engine select pg_catalog.version()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-25 20:57:32,639 [INFO] sqlalchemy.engine.Engine: select pg_catalog.version()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-25 20:57:32,642 INFO sqlalchemy.engine.Engine [raw sql] ()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-25 20:57:32,642 [INFO] sqlalchemy.engine.Engine: [raw sql] ()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-25 20:57:32,644 INFO sqlalchemy.engine.Engine select current_schema()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-25 20:57:32,644 [INFO] sqlalchemy.engine.Engine: select current_schema()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-25 20:57:32,644 INFO sqlalchemy.engine.Engine [raw sql] ()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-25 20:57:32,644 [INFO] sqlalchemy.engine.Engine: [raw sql] ()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-25 20:57:32,651 INFO sqlalchemy.engine.Engine show standard_conforming_strings\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-25 20:57:32,651 [INFO] sqlalchemy.engine.Engine: show standard_conforming_strings\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-25 20:57:32,651 INFO sqlalchemy.engine.Engine [raw sql] ()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-25 20:57:32,651 [INFO] sqlalchemy.engine.Engine: [raw sql] ()\n",
      "2025-09-25 20:57:32,656 [INFO] db.db_connection_pool: DB connection opened\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-25 20:57:32,657 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-25 20:57:32,657 [INFO] sqlalchemy.engine.Engine: BEGIN (implicit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-25 20:57:32,658 INFO sqlalchemy.engine.Engine SELECT 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-25 20:57:32,658 [INFO] sqlalchemy.engine.Engine: SELECT 1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-25 20:57:32,659 INFO sqlalchemy.engine.Engine [generated in 0.00263s] ()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-25 20:57:32,659 [INFO] sqlalchemy.engine.Engine: [generated in 0.00263s] ()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2025-09-25 20:57:32,663 INFO sqlalchemy.engine.Engine ROLLBACK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-25 20:57:32,663 [INFO] sqlalchemy.engine.Engine: ROLLBACK\n",
      "2025-09-25 20:57:32,665 [INFO] db.db_connection_pool: DB connection closed\n"
     ]
    }
   ],
   "source": [
    "# test\n",
    "async with get_conn(db_engine) as conn:\n",
    "    stmt = text(\"SELECT 1\")\n",
    "    result = await conn.execute(stmt)\n",
    "    print(result.scalar())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1edb4422",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-25 20:57:37,324 [INFO] db.db_connection_pool: DB connection opened\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-25 20:57:37,325 INFO sqlalchemy.engine.Engine BEGIN (implicit)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-25 20:57:37,325 [INFO] sqlalchemy.engine.Engine: BEGIN (implicit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-25 20:57:37,326 INFO sqlalchemy.engine.Engine SET search_path TO public, document;\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-25 20:57:37,326 [INFO] sqlalchemy.engine.Engine: SET search_path TO public, document;\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-25 20:57:37,327 INFO sqlalchemy.engine.Engine [generated in 0.00090s] ()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-25 20:57:37,327 [INFO] sqlalchemy.engine.Engine: [generated in 0.00090s] ()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-25 20:57:37,332 INFO sqlalchemy.engine.Engine CREATE SCHEMA IF NOT EXISTS document;\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-25 20:57:37,332 [INFO] sqlalchemy.engine.Engine: CREATE SCHEMA IF NOT EXISTS document;\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-25 20:57:37,332 INFO sqlalchemy.engine.Engine [generated in 0.00092s] ()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-25 20:57:37,332 [INFO] sqlalchemy.engine.Engine: [generated in 0.00092s] ()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-25 20:57:37,334 INFO sqlalchemy.engine.Engine \n",
      "            CREATE TABLE IF NOT EXISTS document.document_chunk (\n",
      "                id              UUID PRIMARY KEY DEFAULT gen_random_uuid(),\n",
      "                embedding       VECTOR(1536),\n",
      "                chunk_text      TEXT,\n",
      "                doc_metadata    JSONB,\n",
      "                file_name       TEXT,\n",
      "                doc_tags        TEXT[],\n",
      "                isActive        BOOLEAN,\n",
      "                version         TEXT,\n",
      "                created_at      TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
      "                updated_at      TIMESTAMP\n",
      "            );\n",
      "        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-25 20:57:37,334 [INFO] sqlalchemy.engine.Engine: \n",
      "            CREATE TABLE IF NOT EXISTS document.document_chunk (\n",
      "                id              UUID PRIMARY KEY DEFAULT gen_random_uuid(),\n",
      "                embedding       VECTOR(1536),\n",
      "                chunk_text      TEXT,\n",
      "                doc_metadata    JSONB,\n",
      "                file_name       TEXT,\n",
      "                doc_tags        TEXT[],\n",
      "                isActive        BOOLEAN,\n",
      "                version         TEXT,\n",
      "                created_at      TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
      "                updated_at      TIMESTAMP\n",
      "            );\n",
      "        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-25 20:57:37,334 INFO sqlalchemy.engine.Engine [generated in 0.00076s] ()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-25 20:57:37,334 [INFO] sqlalchemy.engine.Engine: [generated in 0.00076s] ()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-25 20:57:37,337 INFO sqlalchemy.engine.Engine \n",
      "            CREATE INDEX IF NOT EXISTS documents_embedding_idx\n",
      "            ON document.document_chunk\n",
      "            USING ivfflat (embedding vector_l2_ops)\n",
      "            WITH (lists = 100);\n",
      "        \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-25 20:57:37,337 [INFO] sqlalchemy.engine.Engine: \n",
      "            CREATE INDEX IF NOT EXISTS documents_embedding_idx\n",
      "            ON document.document_chunk\n",
      "            USING ivfflat (embedding vector_l2_ops)\n",
      "            WITH (lists = 100);\n",
      "        \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-25 20:57:37,338 INFO sqlalchemy.engine.Engine [generated in 0.00116s] ()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-25 20:57:37,338 [INFO] sqlalchemy.engine.Engine: [generated in 0.00116s] ()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-09-25 20:57:37,342 INFO sqlalchemy.engine.Engine COMMIT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-09-25 20:57:37,342 [INFO] sqlalchemy.engine.Engine: COMMIT\n",
      "2025-09-25 20:57:37,344 [INFO] __main__: Document schema initialisation finished\n",
      "2025-09-25 20:57:37,346 [INFO] db.db_connection_pool: DB connection closed\n"
     ]
    }
   ],
   "source": [
    "# create schema and tables using sqlalchemy\n",
    "\n",
    "async with get_conn(db_engine) as conn:\n",
    "    # You can wrap everything in an explicit transaction if you want\n",
    "    # an atomic create/commit block.\n",
    "    # the transaction is automatically committed when the block exits\n",
    "    async with conn.begin():   \n",
    "\n",
    "        await conn.execute(text(\"SET search_path TO public, document;\"))\n",
    "\n",
    "        # Create schema\n",
    "        await conn.execute(\n",
    "            text(\"CREATE SCHEMA IF NOT EXISTS document;\")\n",
    "        )\n",
    "\n",
    "        # Create table\n",
    "        create_table_sql = \"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS document.document_chunk (\n",
    "                id              UUID PRIMARY KEY DEFAULT gen_random_uuid(),\n",
    "                embedding       VECTOR(1536),\n",
    "                chunk_text      TEXT,\n",
    "                doc_metadata    JSONB,\n",
    "                file_name       TEXT,\n",
    "                doc_tags        TEXT[],\n",
    "                isActive        BOOLEAN,\n",
    "                version         TEXT,\n",
    "                created_at      TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "                updated_at      TIMESTAMP\n",
    "            );\n",
    "        \"\"\"\n",
    "        await conn.execute(text(create_table_sql))\n",
    "\n",
    "        # Create table index\n",
    "        create_index_sql = \"\"\"\n",
    "            CREATE INDEX IF NOT EXISTS documents_embedding_idx\n",
    "            ON document.document_chunk\n",
    "            USING ivfflat (embedding vector_l2_ops)\n",
    "            WITH (lists = 100);\n",
    "        \"\"\"\n",
    "        await conn.execute(text(create_index_sql))\n",
    "\n",
    "    logger.info(\"Document schema initialisation finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0d4020",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<psycopg2.pool.ThreadedConnectionPool at 0x187e9afbc50>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Batch insert the doc chunks to the document_chunk table\n",
    "async def insert_chunk_batch(conn, rows: Sequence[Dict[str, Any]]):\n",
    "\n",
    "    insert_sql = \"\"\"\n",
    "    INSERT INTO document.document_chunk\n",
    "      (embedding, chunk_text, doc_metadata, file_name, doc_tags, isActive, version, created_at, updated_at)\n",
    "    VALUES\n",
    "      (:embedding, :chunktext, :docmetadata, :filename, :doctags, :isActive, :version, :createdat, :updatedat)\n",
    "    \"\"\"\n",
    "    await conn.execute(text(insert_sql), rows)\n",
    "\n",
    "\n",
    "async def ingest_chunks(engine, docchunks: List[Any]):\n",
    "\n",
    "    async with get_conn(engine) as conn:\n",
    "\n",
    "        # Prepare batches like OKA BATCH_SIZE=10\n",
    "        BATCH_SIZE = 10\n",
    "\n",
    "        for i in range(0, len(docchunks), BATCH_SIZE):\n",
    "            batch = docchunks[i:i+BATCH_SIZE]\n",
    "\n",
    "            texts = [d.chunktext for d in batch]\n",
    "            embeddings = await client.aembed(texts)\n",
    "\n",
    "            rows = []\n",
    "            for d, emb in zip(batch, embeddings):\n",
    "                # Align columns per DocumentChunkBase\n",
    "                doctags = \",\".join(d.tags) if getattr(d, \"tags\", None) else None\n",
    "                rows.append({\n",
    "                    \"embedding\": emb,\n",
    "                    \"chunktext\": d.chunktext,\n",
    "                    \"docmetadata\": json.dumps(d.metadata or {}) if isinstance(d.metadata, dict) else d.metadata,\n",
    "                    \"filename\": getattr(d, \"filename\", None),\n",
    "                    \"doctags\": doctags,\n",
    "                    \"isActive\": getattr(d, \"isActive\", True),\n",
    "                    \"version\": getattr(d, \"version\", None),\n",
    "                    \"createdat\": getattr(d, \"createdat\", current_nz_datetime),\n",
    "                    \"updatedat\": getattr(d, \"updatedat\", None),\n",
    "                })\n",
    "\n",
    "            async with conn.begin():\n",
    "                await insert_chunk_batch(conn, rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "536e9b29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "retriever-techniques-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
